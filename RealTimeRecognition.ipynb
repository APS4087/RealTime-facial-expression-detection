{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> **NOTE:** This code is not working on Google Colab. You need to download the .ipynb file and run it on your local device.\n",
        "\n",
        "Required Dependencies:\n",
        "* OpenCV==4.10.0\n",
        "\n",
        "Model .pth file: https://drive.google.com/file/d/1chn6ohiot6tDgqvGAC3AYsEYhlWOHpbv/view?usp=sharing"
      ],
      "metadata": {
        "id": "vmuMdUbKNnaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb1WulvdNcvk",
        "outputId": "ba626832-cb58-43b7-dfd1-580a6c717eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# If you are using macOS with Apple Silicon:\n",
        "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkeU0ofcNcvm"
      },
      "outputs": [],
      "source": [
        "# Implement VGG5 model\n",
        "\n",
        "class TinyVGG5(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TinyVGG5, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Convolutional Layer Block 1\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 128 -> 64\n",
        "\n",
        "            # Convolutional Layer Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 64 -> 32\n",
        "\n",
        "            # Convolutional Layer Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 32 -> 16\n",
        "\n",
        "            # Convolutional Layer Block 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 16 -> 8\n",
        "\n",
        "            # Convolutional Layer Block 5\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 8 -> 4\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVEI7Cq4Ncvn",
        "outputId": "c9edf054-b7d6-462a-e30f-dd5f12454beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Face Detection Model Loaded Successfully\n",
            "Recognition Model Loaded Successfully\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# initialize the Haar Cascade face detection model\n",
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "print(\"Face Detection Model Loaded Successfully\")\n",
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Contempt\", 2: \"Disgust\", 3: \"Fear\", 4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprise\"}\n",
        "\n",
        "model = TinyVGG5(num_classes=8).to(device)\n",
        "model.load_state_dict(torch.load('tiny_VGG_transfer_learning.pth', map_location=torch.device(device)))\n",
        "print(\"Recognition Model Loaded Successfully\")\n",
        "\n",
        "# Start the webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(3,640) # set Width\n",
        "cap.set(4,480) # set Height\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        cropped_img = cv2.resize(roi_gray, (128, 128))\n",
        "        img_tensor = torch.tensor(cropped_img).float().unsqueeze(0).unsqueeze(0)\n",
        "        img_tensor = img_tensor/255.0\n",
        "        img_tensor = img_tensor.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(img_tensor)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        cv2.putText(frame, emotion_dict[predicted.item()], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    cv2.imshow('frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}